import os
import streamlit as st
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.callbacks.base import BaseCallbackHandler
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.chains import LLMChain
from dotenv import load_dotenv
from datetime import datetime
import pytz
import json

# ç”¨æ–‡ä»¶ä¿å­˜å†å²è®°å½•
HISTORY_FILE = "chat_history.json"


# å…³é—­ huggingface çš„ åŠ é€Ÿtokenizersç¼–ç 
os.environ["TOKENIZERS_PARALLELISM"] = "false"


@st.cache_resource
def load_vectorstore():
    """
    è¿”å›å‘é‡åº“, åŒ…å«chunksã€å¯¹åº”å‘é‡ã€å‘é‡ç´¢å¼•ç»“æ„
    """
    embedding_model = HuggingFaceEmbeddings(model_name="moka-ai/m3e-base")
    return FAISS.load_local(
        "data/faiss_index", embedding_model, allow_dangerous_deserialization=True
    )


def get_beijing_time():
    """
    è¿”å›å½“å‰åŒ—äº¬æ—¶é—´ï¼šæ—¥æœŸ + æ—¶é—´ï¼›æ—¥æœŸï¼›è¿”å› AM æˆ– PM
    """
    tz = pytz.timezone("Asia/Shanghai")
    now = datetime.now(tz)
    return now.strftime("%Y-%m-%d %H:%M"), now.strftime("%p")


# ç»§æ‰¿ langchain æä¾›çš„ BaseCallbackHandlerï¼Œæ¯ç”Ÿæˆä¸€ä¸ªtokenå°±è§¦å‘ä¸€æ¬¡å›è°ƒ
class StreamHandler(BaseCallbackHandler):
    def __init__(self, container):
        self.container = container
        self.text = ""

    def on_llm_new_token(self, token: str, **kwargs) -> None:
        self.text += token
        self.container.markdown(self.text + "â–Œ")  # æ˜¾ç¤ºæµå¼å†…å®¹+å…‰æ ‡


# åŠ è½½æ‰€æœ‰å†å²è®°å½•
def load_all_histories():
    if os.path.exists(HISTORY_FILE):
        with open(HISTORY_FILE, "r", encoding="utf-8") as f:
            return json.load(f)
    return {}


# ä¿å­˜èŠå¤©è®°å½•
def save_all_histories(histories):
    with open(HISTORY_FILE, "w", encoding="utf-8") as f:
        json.dump(histories, f, ensure_ascii=False, indent=2)


def main():
    # åˆå§‹åŒ–å‰ç«¯ç¯å¢ƒ
    st.set_page_config(page_title="åŒ»ç–—åŠ©æ‰‹RAG", layout="wide")
    st.title("åŒ»ç–—ç³»ç»Ÿé—®ç­”åŠ©æ‰‹ ")
    st.write("è¯·è¾“å…¥ä½ çš„é—®é¢˜ï¼Œæ”¯æŒå¤šè½®è¿½é—®ã€‚")

    # è®¾ç½®Sidebarç”¨äºç®¡ç†å¤šä¸ªå¯¹è¯
    st.sidebar.title("ğŸ§¾ å¯¹è¯ç®¡ç†")

    # åŠ è½½å…¨éƒ¨å†å²
    all_histories = load_all_histories()

    # è·å–æ‰€æœ‰å¯¹è¯ IDï¼ˆé»˜è®¤æ ‡é¢˜æ˜¾ç¤ºå‰å‡ å¥ï¼‰
    chat_ids = list(all_histories.keys())
    chat_titles = [
        all_histories[cid][0]["content"][:20] if all_histories[cid] else cid
        for cid in chat_ids
    ]

    # æ˜¾ç¤ºæ‰€æœ‰å·²æœ‰å¯¹è¯ä½œä¸ºæŒ‰é’®
    for cid, title in zip(chat_ids, chat_titles):
        if st.sidebar.button(title, key=f"chat_btn_{cid}"):
            st.session_state["chat_id"] = cid
            st.session_state["history"] = all_histories.get(cid, [])
            st.rerun()

    # æ–°å»ºå¯¹è¯æŒ‰é’®
    if st.sidebar.button("â• æ–°å»ºå¯¹è¯"):
        new_id = f"chat_{len(chat_ids)+1}"
        all_histories[new_id] = []
        save_all_histories(all_histories)
        st.session_state["chat_id"] = new_id
        st.session_state["history"] = []
        st.rerun()

    # åŠ è½½ FAISS
    vectorstore = load_vectorstore()
    load_dotenv()

    # åˆå§‹åŒ– LLM å’Œ prompt
    llm = ChatOpenAI(
        openai_api_key=os.getenv("DEEPSEEK_API_KEY"),
        base_url="https://api.deepseek.com/v1",
        model="deepseek-chat",
        streaming=True,
    )

    prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                "ä½ æ˜¯æœåŠ¡äºç—…æ‚£çš„åŒ»ç–—åŠ©æ‰‹ï¼Œè¯·æ¸©æŸ”è€å¿ƒã€ä¾æ®äº‹å®ä»¥åŠå†å²å¯¹è¯å›ç­”ç”¨æˆ·é—®é¢˜ï¼Œå¦‚æœç”¨æˆ·æç¤ºä¸æ¸…æ™°ï¼Œè¯·çŒœæµ‹ç”¨æˆ·æ¥ä¸‹æ¥æƒ³è¯´çš„å†…å®¹å¹¶ç»™åˆ°ç”¨æˆ·å¼•å¯¼ã€‚",
            ),
            MessagesPlaceholder(variable_name="history"),
            (
                "user",
                "{input}\nè¯·ç»“åˆå¤‡æ³¨ï¼Œè¯¦ç»†è¯´æ˜ä½ æ˜¯å¦‚ä½•å¾—å‡ºjie'lunçš„ï¼Œå±•ç¤ºä½ çš„æ€è€ƒè¿‡ç¨‹ã€‚å¦‚æœæ£€ç´¢å†…å®¹æ— å…³ï¼Œè¯·è¯´æ˜ç†ç”±ã€‚",
            ),
        ]
    )

    # ç”¨æˆ·é¦–æ¬¡è¿›å…¥streamlitæ—¶ï¼Œåˆå§‹åŒ– Streamlit èŠå¤© ä»¥åŠ å¯¹è¯å†å²
    if "chat_id" not in st.session_state:
        if chat_ids:
            st.session_state["chat_id"] = chat_ids[0]
            st.session_state["history"] = all_histories.get(chat_ids[0], [])
        else:
            st.session_state["chat_id"] = "chat_1"
            st.session_state["history"] = []

    # è¾“å…¥æ¡†äº¤äº’
    user_input = st.chat_input("è¯·è¾“å…¥ä½ çš„é—®é¢˜...")

    # å±•ç¤ºæ‰€æœ‰å†å²æ¶ˆæ¯ä»¥åŠæ¨ç†è¿‡ç¨‹
    history = st.session_state["history"]
    for i, msg in enumerate(history):
        with st.chat_message("user" if msg["role"] == "user" else "assistant"):
            st.write(msg["content"])

            # å¦‚æœå½“å‰æ˜¯æœ€åä¸€æ¡ assistant æ¶ˆæ¯ï¼Œå¯é€‰æ‹©å±•ç¤ºæ¨ç†ä¿¡æ¯
            is_last = i == len(history) - 1
            is_assistant = msg["role"] == "assistant"
            if is_last and is_assistant and "last_answer_meta" in st.session_state:
                meta = st.session_state["last_answer_meta"]
                with st.expander("ğŸ§  æ¨¡å‹æ¨ç†è¿‡ç¨‹ï¼ˆç‚¹å‡»å±•å¼€ï¼‰"):
                    st.markdown("**åŸå§‹ç”¨æˆ·é—®é¢˜ï¼š**")
                    st.code(meta["user_input"], language="markdown")

                    st.markdown("**æ£€ç´¢åˆ°çš„å†…å®¹ç‰‡æ®µï¼š**")
                    for j, chunk in enumerate(meta["dense_results"]):
                        st.markdown(f"**ç‰‡æ®µ {j+1}:**")
                        st.code(chunk, language="markdown")

                    st.markdown("**ä¼ ç»™æ¨¡å‹çš„å®Œæ•´ Prompt è¾“å…¥ï¼š**")
                    st.code(meta["full_input"], language="markdown")

                    st.markdown("**æœ€ç»ˆå›ç­”ï¼š**")
                    st.code(meta["answer"], language="markdown")

                # å±•ç¤ºåç«‹å³æ¸…é™¤
                del st.session_state["last_answer_meta"]

    if user_input:
        # å±•ç¤ºç”¨æˆ·æé—®
        st.chat_message("user").write(user_input)

        # è·å–å½“å‰æ—¶é—´ä¿¡æ¯
        beijing_full_time, time_period = get_beijing_time()

        # è¿›è¡Œç¨ å¯†æ£€ç´¢ chunk
        dense_results = vectorstore.similarity_search(user_input, k=10)
        context = "\n\n".join([doc.page_content for doc in dense_results])

        # æ‹¼æ¥ prompt è¾“å…¥
        full_input = f"å½“å‰åŒ—äº¬æ—¶é—´ï¼š{beijing_full_time, time_period}\n\né—®é¢˜ï¼š{user_input}\n\nä»¥ä¸‹æ˜¯å¯èƒ½ç›¸å…³çš„åŒ»ç”Ÿæ’ç­ä¿¡æ¯ï¼š\n{context}"

        # åª append æœ€åŸå§‹çš„ç”¨æˆ·è¾“å…¥åˆ°å†å²ä¸­
        st.session_state["history"].append({"role": "user", "content": user_input})

        # åˆ›å»º assistant å¯¹è¯å®¹å™¨å¹¶æµå¼è¾“å‡º
        with st.chat_message("assistant"):
            # åˆ›å»ºæµå¼è¾“å‡ºå®¹å™¨
            msg_container = st.empty()

            # åˆå§‹åŒ–æµå¼ handler
            stream_handler = StreamHandler(msg_container)
            llm.callbacks = [stream_handler]

            # æ„é€  LLM è¾“å…¥ï¼ˆå†å²è®°å½•+æœ¬è½®å®Œæ•´ä¸Šä¸‹æ–‡ï¼‰ï¼Œå¹¶è¿›è¡Œæ¨ç†
            chain = prompt | llm
            inputs = {"history": st.session_state["history"], "input": full_input}
            answer = chain.invoke(inputs).content

            # å‚¨å­˜æ¨ç†ç»†èŠ‚ï¼ˆåªèƒ½ä¸€è½®å±•ç¤ºï¼‰
            st.session_state["last_answer_meta"] = {
                "user_input": user_input,
                "dense_results": [doc.page_content for doc in dense_results],
                "full_input": full_input,
                "answer": answer,
            }

        # æ·»åŠ å›ç­”
        st.session_state["history"].append({"role": "assistant", "content": answer})

        # ä¿å­˜åˆ°å¤šä¼šè¯æ€»è®°å½•ä¸­
        all_histories[st.session_state["chat_id"]] = st.session_state["history"]
        save_all_histories(all_histories)

        # åˆ·æ–°ï¼Œæ˜¾ç¤ºä¼šè¯ä¿¡æ¯
        st.rerun()


if __name__ == "__main__":
    main()
